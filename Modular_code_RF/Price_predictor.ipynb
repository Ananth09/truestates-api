{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acd8d9e9-fe26-4483-820d-47f65c25992b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Base Directory:\n",
      " C:\\Users\\pooja\\Downloads\\FLIPOSE_DATA\\Modular_Code\\Modular_code with 18 areas_RF\n",
      "\n",
      "Files in directory: ['.ipynb_checkpoints', 'historical_data.csv', 'historical_df.csv', 'model_columns_Al Barsha South Fifth.pkl', 'model_columns_Al Barsha South Fourth.pkl', 'model_columns_Al Barshaa South Third.pkl', 'model_columns_Al Hebiah Fourth.pkl', 'model_columns_Al Khairan First.pkl', 'model_columns_Al Merkadh.pkl', 'model_columns_Al Thanyah Fifth.pkl', 'model_columns_Al Yelayiss 2.pkl', 'model_columns_Burj Khalifa.pkl', 'model_columns_Business Bay.pkl', 'model_columns_Hadaeq Sheikh Mohammed Bin Rashid.pkl', 'model_columns_Jabal Ali First.pkl', 'model_columns_Madinat Al Mataar.pkl', 'model_columns_Madinat Dubai Almelaheyah.pkl', 'model_columns_Marsa Dubai.pkl', \"model_columns_Me'Aisem First.pkl\", 'model_columns_Nadd Hessa.pkl', 'model_columns_Wadi Al Safa 5.pkl', 'Price_predictor.ipynb', 'rf_model_Al Barsha South Fifth.pkl', 'rf_model_Al Merkadh.pkl', 'rf_model_Madinat Dubai Almelaheyah.pkl', 'Sarima_forecast_6M.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ BASE DIRECTORY – MODELS PER AREA\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = os.path.join(\n",
    "    os.path.expanduser(\"~\"),\n",
    "    \"Downloads\",\n",
    "    \"FLIPOSE_DATA\",\n",
    "    \"Modular_Code\",\n",
    "    \"Modular_code with 18 areas_RF\"\n",
    ")\n",
    "\n",
    "print(\"Using Base Directory:\\n\", BASE_DIR)\n",
    "print(\"\\nFiles in directory:\", os.listdir(BASE_DIR))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ LOAD TRAINING COLUMNS (per area)\n",
    "# ============================================================\n",
    "\n",
    "def load_columns(area_name_en):\n",
    "    \"\"\"\n",
    "    Load the expected training columns for a given area.\n",
    "    Case-insensitive and handles spaces/underscores in names.\n",
    "    \"\"\"\n",
    "    # Convert underscores → spaces for consistency\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "\n",
    "    # Build expected filename pattern\n",
    "    safe = area_name_en.replace(\" \", \"_\")\n",
    "    expected = f\"model_columns_{safe}.pkl\"\n",
    "\n",
    "    # Search ignoring case\n",
    "    for f in os.listdir(BASE_DIR):\n",
    "        if f.lower() == expected.lower():\n",
    "            with open(os.path.join(BASE_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ model_columns file not found for area '{area_name_en}'\\n\"\n",
    "        f\"Expected filename (case-insensitive): {expected}\\n\"\n",
    "        f\"Available files: {os.listdir(BASE_DIR)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ LOAD MODEL (per area)\n",
    "# ============================================================\n",
    "\n",
    "def load_model(area_name_en):\n",
    "    \"\"\"\n",
    "    Load the trained model for a given area.\n",
    "    Case-insensitive and handles spaces/underscores in names.\n",
    "    \"\"\"\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "\n",
    "    safe = area_name_en.replace(\" \", \"_\")\n",
    "    expected = f\"rf_model_{safe}.pkl\"\n",
    "\n",
    "    for f in os.listdir(BASE_DIR):\n",
    "        if f.lower() == expected.lower():\n",
    "            with open(os.path.join(BASE_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ Model file not found for area '{area_name_en}'\\n\"\n",
    "        f\"Expected filename (case-insensitive): {expected}\\n\"\n",
    "        f\"Available files: {os.listdir(BASE_DIR)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ MAIN PREDICTION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def predict_with_area(input_data, forecast_df, historic_df):\n",
    "    area = input_data[\"area_name_en\"].replace(\"_\", \" \").strip()\n",
    "\n",
    "    # Step 1: Load model + expected columns\n",
    "    train_columns = load_columns(area)\n",
    "    model = load_model(area)\n",
    "\n",
    "    # Step 2: One-hot encode input\n",
    "    temp = pd.DataFrame([input_data])\n",
    "    temp[\"area_name_en\"] = area  # ensure correct area\n",
    "    temp = pd.get_dummies(temp)\n",
    "\n",
    "    # Add missing training columns\n",
    "    for col in train_columns:\n",
    "        if col not in temp.columns:\n",
    "            temp[col] = 0\n",
    "\n",
    "    temp = temp[train_columns]\n",
    "\n",
    "    # Step 3: Predict median price\n",
    "    predicted_price = model.predict(temp)[0]\n",
    "    print(\"Raw Model Prediction:\", predicted_price)\n",
    "\n",
    "    # Step 4: Clean forecast_df column names\n",
    "    if \"year_month\" in forecast_df.columns:\n",
    "        forecast_df = forecast_df.rename(columns={\"year_month\": \"month\"})\n",
    "\n",
    "    gf = forecast_df[forecast_df[\"area_name_en\"] == area].copy()\n",
    "    gf[\"median_price\"] = predicted_price * gf[\"growth_factor\"]\n",
    "\n",
    "    # Step 5: Clean historic_df column names\n",
    "    if \"year_month\" in historic_df.columns:\n",
    "        historic_df = historic_df.rename(columns={\"year_month\": \"month\"})\n",
    "    if \"meter_sale_price\" in historic_df.columns:\n",
    "        historic_df = historic_df.rename(columns={\"meter_sale_price\": \"median_price\"})\n",
    "\n",
    "    # Step 6: Filter historic for specific area\n",
    "    if historic_df.empty or \"area_name_en\" not in historic_df.columns:\n",
    "        print(f\"⚠️ No historic data found for {area}.\")\n",
    "        historic = pd.DataFrame()\n",
    "    else:\n",
    "        historic = historic_df[historic_df[\"area_name_en\"] == area].copy()\n",
    "\n",
    "    # Step 7: LOWESS smoothing\n",
    "    if not historic.empty:\n",
    "        historic = historic.sort_values(\"month\")\n",
    "        x = historic[\"month\"].astype(\"int64\") / 10**9\n",
    "\n",
    "        smoothed = lowess(\n",
    "            endog=historic[\"median_price\"],\n",
    "            exog=x,\n",
    "            frac=0.03\n",
    "        )\n",
    "\n",
    "        historic[\"median_price\"] = smoothed[:, 1]\n",
    "        historic.loc[historic.index[-1], \"median_price\"] = predicted_price\n",
    "\n",
    "    # Step 8: Clean & combine outputs\n",
    "    for df_ in [historic, gf]:\n",
    "        if \"area_name_en\" in df_.columns:\n",
    "            df_.drop(columns=[\"area_name_en\"], inplace=True)\n",
    "\n",
    "    final_df = pd.concat([historic, gf], ignore_index=True)\n",
    "    final_df = final_df[[\"month\", \"median_price\"]]\n",
    "    final_df = final_df.sort_values(\"month\").reset_index(drop=True)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26613996-f0f6-4102-a9f3-0485a05720da",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "❌ model_columns file not found for area 'Al Barsha South Fifth'\nExpected filename (case-insensitive): model_columns_Al_Barsha_South_Fifth.pkl\nAvailable files: ['.ipynb_checkpoints', 'historical_data.csv', 'historical_df.csv', 'model_columns_Al Barsha South Fifth.pkl', 'model_columns_Al Barsha South Fourth.pkl', 'model_columns_Al Barshaa South Third.pkl', 'model_columns_Al Hebiah Fourth.pkl', 'model_columns_Al Khairan First.pkl', 'model_columns_Al Merkadh.pkl', 'model_columns_Al Thanyah Fifth.pkl', 'model_columns_Al Yelayiss 2.pkl', 'model_columns_Burj Khalifa.pkl', 'model_columns_Business Bay.pkl', 'model_columns_Hadaeq Sheikh Mohammed Bin Rashid.pkl', 'model_columns_Jabal Ali First.pkl', 'model_columns_Madinat Al Mataar.pkl', 'model_columns_Madinat Dubai Almelaheyah.pkl', 'model_columns_Marsa Dubai.pkl', \"model_columns_Me'Aisem First.pkl\", 'model_columns_Nadd Hessa.pkl', 'model_columns_Wadi Al Safa 5.pkl', 'Price_predictor.ipynb', 'rf_model_Al Barsha South Fifth.pkl', 'rf_model_Al Merkadh.pkl', 'rf_model_Madinat Dubai Almelaheyah.pkl', 'Sarima_forecast_6M.csv']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 19\u001b[0m\n\u001b[0;32m      6\u001b[0m historic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistorical_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m input_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea_name_en\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAl_Barsha_South_Fifth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocedure_area\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melevator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m }\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m predict_with_area(input_data, forecast_df, historic_df)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "Cell \u001b[1;32mIn[30], line 85\u001b[0m, in \u001b[0;36mpredict_with_area\u001b[1;34m(input_data, forecast_df, historic_df)\u001b[0m\n\u001b[0;32m     82\u001b[0m area \u001b[38;5;241m=\u001b[39m input_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea_name_en\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Step 1: Load model + expected columns\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m train_columns \u001b[38;5;241m=\u001b[39m load_columns(area)\n\u001b[0;32m     86\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(area)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Step 2: One-hot encode input\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 44\u001b[0m, in \u001b[0;36mload_columns\u001b[1;34m(area_name_en)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, f), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     42\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ model_columns file not found for area \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marea_name_en\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected filename (case-insensitive): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mlistdir(BASE_DIR)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m )\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: ❌ model_columns file not found for area 'Al Barsha South Fifth'\nExpected filename (case-insensitive): model_columns_Al_Barsha_South_Fifth.pkl\nAvailable files: ['.ipynb_checkpoints', 'historical_data.csv', 'historical_df.csv', 'model_columns_Al Barsha South Fifth.pkl', 'model_columns_Al Barsha South Fourth.pkl', 'model_columns_Al Barshaa South Third.pkl', 'model_columns_Al Hebiah Fourth.pkl', 'model_columns_Al Khairan First.pkl', 'model_columns_Al Merkadh.pkl', 'model_columns_Al Thanyah Fifth.pkl', 'model_columns_Al Yelayiss 2.pkl', 'model_columns_Burj Khalifa.pkl', 'model_columns_Business Bay.pkl', 'model_columns_Hadaeq Sheikh Mohammed Bin Rashid.pkl', 'model_columns_Jabal Ali First.pkl', 'model_columns_Madinat Al Mataar.pkl', 'model_columns_Madinat Dubai Almelaheyah.pkl', 'model_columns_Marsa Dubai.pkl', \"model_columns_Me'Aisem First.pkl\", 'model_columns_Nadd Hessa.pkl', 'model_columns_Wadi Al Safa 5.pkl', 'Price_predictor.ipynb', 'rf_model_Al Barsha South Fifth.pkl', 'rf_model_Al Merkadh.pkl', 'rf_model_Madinat Dubai Almelaheyah.pkl', 'Sarima_forecast_6M.csv']"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. USER INPUT TEST\n",
    "# ============================================================\n",
    "\n",
    "forecast_df = pd.read_csv(\"Sarima_forecast_6M.csv\")\n",
    "historic_df = pd.read_csv(\"historical_df.csv\")\n",
    "\n",
    "input_data = {\n",
    "    \"area_name_en\": \"Al_Barsha_South_Fifth\",\n",
    "    \"procedure_area\": 100,\n",
    "    \"has_parking\": 1,\n",
    "    \"floor_bin\": \"41-50\",\n",
    "    \"rooms_en\": \"1BR\",\n",
    "    \"swimming_pool\": 1,\n",
    "    \"balcony\": 1,\n",
    "    \"elevator\": 1\n",
    "}\n",
    "\n",
    "output = predict_with_area(input_data, forecast_df, historic_df)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eab7c263-60d1-4efd-a3cd-8d0cd351873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Base Directory:\n",
      " C:\\Users\\pooja\\Downloads\\FLIPOSE_DATA\\Modular_Code\\Modular_code with 18 areas_RF\n",
      "\n",
      "Files in directory: ['.ipynb_checkpoints', 'historical_data.csv', 'historical_df.csv', 'model_columns_Al Barsha South Fifth.pkl', 'model_columns_Al Barsha South Fourth.pkl', 'model_columns_Al Barshaa South Third.pkl', 'model_columns_Al Hebiah Fourth.pkl', 'model_columns_Al Khairan First.pkl', 'model_columns_Al Merkadh.pkl', 'model_columns_Al Thanyah Fifth.pkl', 'model_columns_Al Yelayiss 2.pkl', 'model_columns_Burj Khalifa.pkl', 'model_columns_Business Bay.pkl', 'model_columns_Hadaeq Sheikh Mohammed Bin Rashid.pkl', 'model_columns_Jabal Ali First.pkl', 'model_columns_Madinat Al Mataar.pkl', 'model_columns_Madinat Dubai Almelaheyah.pkl', 'model_columns_Marsa Dubai.pkl', \"model_columns_Me'Aisem First.pkl\", 'model_columns_Nadd Hessa.pkl', 'model_columns_Wadi Al Safa 5.pkl', 'Price_predictor.ipynb', 'rf_model_Al Barsha South Fifth.pkl', 'rf_model_Al Merkadh.pkl', 'rf_model_Madinat Dubai Almelaheyah.pkl', 'Sarima_forecast_6M.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ BASE DIRECTORY – MODELS PER AREA\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = os.path.join(\n",
    "    os.path.expanduser(\"~\"),\n",
    "    \"Downloads\",\n",
    "    \"FLIPOSE_DATA\",\n",
    "    \"Modular_Code\",\n",
    "    \"Modular_code with 18 areas_RF\"\n",
    ")\n",
    "\n",
    "print(\"Using Base Directory:\\n\", BASE_DIR)\n",
    "print(\"\\nFiles in directory:\", os.listdir(BASE_DIR))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ LOAD TRAINING COLUMNS (per area)\n",
    "# ============================================================\n",
    "\n",
    "def load_columns(area_name_en):\n",
    "    \"\"\"\n",
    "    Load the expected training columns for a given area.\n",
    "    Handles spaces/underscores and is case-insensitive.\n",
    "    \"\"\"\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "    \n",
    "    for f in os.listdir(BASE_DIR):\n",
    "        if f.lower() == f\"model_columns_{area_name_en}.pkl\".lower() or \\\n",
    "           f.lower() == f\"model_columns_{area_name_en.replace(' ', '_')}.pkl\".lower():\n",
    "            with open(os.path.join(BASE_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ model_columns file not found for area '{area_name_en}'\\n\"\n",
    "        f\"Available files: {os.listdir(BASE_DIR)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ LOAD MODEL (per area)\n",
    "# ============================================================\n",
    "\n",
    "def load_model(area_name_en):\n",
    "    \"\"\"\n",
    "    Load the trained model for a given area.\n",
    "    Handles spaces/underscores and is case-insensitive.\n",
    "    \"\"\"\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "    \n",
    "    for f in os.listdir(BASE_DIR):\n",
    "        if f.lower() == f\"rf_model_{area_name_en}.pkl\".lower() or \\\n",
    "           f.lower() == f\"rf_model_{area_name_en.replace(' ', '_')}.pkl\".lower():\n",
    "            with open(os.path.join(BASE_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ Model file not found for area '{area_name_en}'\\n\"\n",
    "        f\"Available files: {os.listdir(BASE_DIR)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ MAIN PREDICTION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def predict_with_area(input_data, forecast_df, historic_df):\n",
    "    area = input_data[\"area_name_en\"].replace(\"_\", \" \").strip()\n",
    "\n",
    "    # Step 1: Load model + expected columns\n",
    "    train_columns = load_columns(area)\n",
    "    model = load_model(area)\n",
    "\n",
    "    # Step 2: One-hot encode input\n",
    "    temp = pd.DataFrame([input_data])\n",
    "    temp[\"area_name_en\"] = area  # ensure correct area\n",
    "    temp = pd.get_dummies(temp)\n",
    "\n",
    "    # Add missing training columns\n",
    "    for col in train_columns:\n",
    "        if col not in temp.columns:\n",
    "            temp[col] = 0\n",
    "\n",
    "    temp = temp[train_columns]\n",
    "\n",
    "    # Step 3: Predict median price\n",
    "    predicted_price = model.predict(temp)[0]\n",
    "    print(\"Raw Model Prediction:\", predicted_price)\n",
    "\n",
    "    # Step 4: Clean forecast_df column names\n",
    "    #if \"year_month\" in forecast_df.columns:\n",
    "        #forecast_df = forecast_df.rename(columns={\"year_month\": \"month\"})\n",
    "\n",
    "    gf = forecast_df[forecast_df[\"area_name_en\"].replace(\"_\", \" \") == area].copy()\n",
    "    gf[\"median_price\"] = predicted_price * gf[\"growth_factor\"]\n",
    "\n",
    "    # Step 5: Clean historic_df column names\n",
    "    if \"year_month\" in historic_df.columns:\n",
    "        historic_df = historic_df.rename(columns={\"year_month\": \"month\"})\n",
    "    if \"meter_sale_price\" in historic_df.columns:\n",
    "        historic_df = historic_df.rename(columns={\"meter_sale_price\": \"median_price\"})\n",
    "\n",
    "    # Step 6: Filter historic for specific area\n",
    "    if historic_df.empty or \"area_name_en\" not in historic_df.columns:\n",
    "        print(f\"⚠️ No historic data found for {area}.\")\n",
    "        historic = pd.DataFrame()\n",
    "    else:\n",
    "        historic = historic_df[historic_df[\"area_name_en\"].replace(\"_\", \" \") == area].copy()\n",
    "\n",
    "    # Step 7: LOWESS smoothing\n",
    "    if not historic.empty:\n",
    "        historic = historic.sort_values(\"month\")\n",
    "        x = historic[\"month\"].astype(\"int64\") / 10**9\n",
    "\n",
    "        smoothed = lowess(\n",
    "            endog=historic[\"median_price\"],\n",
    "            exog=x,\n",
    "            frac=0.03\n",
    "        )\n",
    "\n",
    "        historic[\"median_price\"] = smoothed[:, 1]\n",
    "        historic.loc[historic.index[-1], \"median_price\"] = predicted_price\n",
    "\n",
    "    # Step 8: Clean & combine outputs\n",
    "    for df_ in [historic, gf]:\n",
    "        if \"area_name_en\" in df_.columns:\n",
    "            df_.drop(columns=[\"area_name_en\"], inplace=True)\n",
    "\n",
    "    final_df = pd.concat([historic, gf], ignore_index=True)\n",
    "    final_df = final_df[[\"month\", \"median_price\"]]\n",
    "    final_df = final_df.sort_values(\"month\").reset_index(drop=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7977baf7-e839-456b-8996-e36633c232b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Model Prediction: 8726.947816469818\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '2020-01-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 19\u001b[0m\n\u001b[0;32m      6\u001b[0m historic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistorical_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m input_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea_name_en\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAl_Barsha_South_Fifth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocedure_area\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melevator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m }\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m predict_with_area(input_data, forecast_df, historic_df)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "Cell \u001b[1;32mIn[39], line 118\u001b[0m, in \u001b[0;36mpredict_with_area\u001b[1;34m(input_data, forecast_df, historic_df)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m historic\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m    117\u001b[0m     historic \u001b[38;5;241m=\u001b[39m historic\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 118\u001b[0m     x \u001b[38;5;241m=\u001b[39m historic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m    120\u001b[0m     smoothed \u001b[38;5;241m=\u001b[39m lowess(\n\u001b[0;32m    121\u001b[0m         endog\u001b[38;5;241m=\u001b[39mhistoric[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_price\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    122\u001b[0m         exog\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    123\u001b[0m         frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[0;32m    126\u001b[0m     historic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_price\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m smoothed[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    432\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    433\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    434\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    435\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[0;32m    436\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '2020-01-01'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5️⃣ USER INPUT TEST\n",
    "# ============================================================\n",
    "\n",
    "forecast_df = pd.read_csv(\"Sarima_forecast_6M.csv\")\n",
    "historic_df = pd.read_csv(\"historical_df.csv\")\n",
    "\n",
    "input_data = {\n",
    "    \"area_name_en\": \"Al_Barsha_South_Fifth\",\n",
    "    \"procedure_area\": 150,\n",
    "    \"has_parking\": 1,\n",
    "    \"floor_bin\": \"41-50\",\n",
    "    \"rooms_en\": \"4BR\",\n",
    "    \"swimming_pool\": 1,\n",
    "    \"balcony\": 1,\n",
    "    \"elevator\": 1\n",
    "}\n",
    "\n",
    "output = predict_with_area(input_data, forecast_df, historic_df)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df1186d8-846f-46b7-9cfc-57414ca483b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>area_name_en</th>\n",
       "      <th>month</th>\n",
       "      <th>median_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>817</td>\n",
       "      <td>Burj Khalifa</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>17944.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>818</td>\n",
       "      <td>Madinat Al Mataar</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>6536.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>819</td>\n",
       "      <td>Wadi Al Safa 5</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4918.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>820</td>\n",
       "      <td>Me'Aisem First</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>6401.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821</td>\n",
       "      <td>Marsa Dubai</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>12994.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>2067</td>\n",
       "      <td>Nadd Hessa</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>9486.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>2068</td>\n",
       "      <td>Al Barshaa South Third</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>15360.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>2069</td>\n",
       "      <td>Al Barsha South Fourth</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>15463.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>2070</td>\n",
       "      <td>Burj Khalifa</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>34356.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2071</td>\n",
       "      <td>Wadi Al Safa 5</td>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>15825.035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1255 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0            area_name_en       month  median_price\n",
       "0            817            Burj Khalifa  2020-01-01     17944.075\n",
       "1            818       Madinat Al Mataar  2020-01-01      6536.165\n",
       "2            819          Wadi Al Safa 5  2020-01-01      4918.840\n",
       "3            820          Me'Aisem First  2020-01-01      6401.910\n",
       "4            821             Marsa Dubai  2020-01-01     12994.990\n",
       "...          ...                     ...         ...           ...\n",
       "1250        2067              Nadd Hessa  2025-11-01      9486.420\n",
       "1251        2068  Al Barshaa South Third  2025-11-01     15360.500\n",
       "1252        2069  Al Barsha South Fourth  2025-11-01     15463.110\n",
       "1253        2070            Burj Khalifa  2025-11-01     34356.810\n",
       "1254        2071          Wadi Al Safa 5  2025-11-01     15825.035\n",
       "\n",
       "[1255 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a67b872-d8e4-4f01-a879-e4d00ecb5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Base Directory:\n",
      " C:\\Users\\pooja\\Downloads\\FLIPOSE_DATA\\Modular_Code\\Modular_code with 18 areas_RF\n",
      "\n",
      "Files in directory: ['.ipynb_checkpoints', 'historical_data.csv', 'historical_df.csv', 'model_columns_Al Barsha South Fifth.pkl', 'model_columns_Al Barsha South Fourth.pkl', 'model_columns_Al Barshaa South Third.pkl', 'model_columns_Al Hebiah Fourth.pkl', 'model_columns_Al Khairan First.pkl', 'model_columns_Al Merkadh.pkl', 'model_columns_Al Thanyah Fifth.pkl', 'model_columns_Al Yelayiss 2.pkl', 'model_columns_Burj Khalifa.pkl', 'model_columns_Business Bay.pkl', 'model_columns_Hadaeq Sheikh Mohammed Bin Rashid.pkl', 'model_columns_Jabal Ali First.pkl', 'model_columns_Madinat Al Mataar.pkl', 'model_columns_Madinat Dubai Almelaheyah.pkl', 'model_columns_Marsa Dubai.pkl', \"model_columns_Me'Aisem First.pkl\", 'model_columns_Nadd Hessa.pkl', 'model_columns_Wadi Al Safa 5.pkl', 'Price_predictor.ipynb', 'rf_model_Al Barsha South Fifth.pkl', 'rf_model_Al Merkadh.pkl', 'rf_model_Madinat Dubai Almelaheyah.pkl', 'Sarima_forecast_6M.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ BASE DIRECTORY – MODELS PER AREA\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = os.path.join(\n",
    "    os.path.expanduser(\"~\"),\n",
    "    \"Downloads\",\n",
    "    \"FLIPOSE_DATA\",\n",
    "    \"Modular_Code\",\n",
    "    \"Modular_code with 18 areas_RF\"\n",
    ")\n",
    "\n",
    "print(\"Using Base Directory:\\n\", BASE_DIR)\n",
    "print(\"\\nFiles in directory:\", os.listdir(BASE_DIR))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ LOAD TRAINING COLUMNS (per area)\n",
    "# ============================================================\n",
    "\n",
    "def load_columns(area_name_en):\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "    \n",
    "    for f in os.listdir(BASE_DIR):\n",
    "        if f.lower() == f\"model_columns_{area_name_en}.pkl\".lower() or \\\n",
    "           f.lower() == f\"model_columns_{area_name_en.replace(' ', '_')}.pkl\".lower():\n",
    "            with open(os.path.join(BASE_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ model_columns file not found for area '{area_name_en}'\\n\"\n",
    "        f\"Available files: {os.listdir(BASE_DIR)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ LOAD MODEL (per area)\n",
    "# ============================================================\n",
    "\n",
    "def load_model(area_name_en):\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "    \n",
    "    for f in os.listdir(BASE_DIR):\n",
    "        if f.lower() == f\"rf_model_{area_name_en}.pkl\".lower() or \\\n",
    "           f.lower() == f\"rf_model_{area_name_en.replace(' ', '_')}.pkl\".lower():\n",
    "            with open(os.path.join(BASE_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ Model file not found for area '{area_name_en}'\\n\"\n",
    "        f\"Available files: {os.listdir(BASE_DIR)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ MAIN PREDICTION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def predict_with_area(input_data, forecast_df, historic_df):\n",
    "    area = input_data[\"area_name_en\"].replace(\"_\", \" \").strip()\n",
    "\n",
    "    # Step 1: Load model + expected columns\n",
    "    train_columns = load_columns(area)\n",
    "    model = load_model(area)\n",
    "\n",
    "    # Step 2: One-hot encode input\n",
    "    temp = pd.DataFrame([input_data])\n",
    "    temp[\"area_name_en\"] = area  # ensure correct area\n",
    "    temp = pd.get_dummies(temp)\n",
    "\n",
    "    # Add missing training columns\n",
    "    for col in train_columns:\n",
    "        if col not in temp.columns:\n",
    "            temp[col] = 0\n",
    "\n",
    "    temp = temp[train_columns]\n",
    "\n",
    "    # Step 3: Predict median price\n",
    "    predicted_price = model.predict(temp)[0]\n",
    "    print(\"Raw Model Prediction:\", predicted_price)\n",
    "\n",
    "    # Step 4: Clean forecast_df column names\n",
    "    if \"year_month\" in forecast_df.columns:\n",
    "        forecast_df = forecast_df.rename(columns={\"year_month\": \"month\"})\n",
    "\n",
    "    gf = forecast_df[forecast_df[\"area_name_en\"].replace(\"_\", \" \") == area].copy()\n",
    "    gf[\"median_price\"] = predicted_price * gf[\"growth_factor\"]\n",
    "\n",
    "    # Step 5: Clean historic_df column names\n",
    "    if \"year_month\" in historic_df.columns:\n",
    "        historic_df = historic_df.rename(columns={\"year_month\": \"month\"})\n",
    "    if \"meter_sale_price\" in historic_df.columns:\n",
    "        historic_df = historic_df.rename(columns={\"meter_sale_price\": \"median_price\"})\n",
    "\n",
    "    # Step 6: Filter historic for specific area\n",
    "    if historic_df.empty or \"area_name_en\" not in historic_df.columns:\n",
    "        print(f\"⚠️ No historic data found for {area}.\")\n",
    "        historic = pd.DataFrame()\n",
    "    else:\n",
    "        historic = historic_df[historic_df[\"area_name_en\"].replace(\"_\", \" \") == area].copy()\n",
    "\n",
    "    # Step 7: LOWESS smoothing\n",
    "    if not historic.empty:\n",
    "        historic = historic.sort_values(\"month\")\n",
    "        # Convert to datetime first\n",
    "        historic[\"month\"] = pd.to_datetime(historic[\"month\"])\n",
    "        x = historic[\"month\"].astype(\"int64\") #// 10**9  # UNIX timestamp in seconds\n",
    "\n",
    "        smoothed = lowess(\n",
    "            endog=historic[\"median_price\"],\n",
    "            exog=x,\n",
    "            frac=0.03\n",
    "        )\n",
    "\n",
    "        historic[\"median_price\"] = smoothed[:, 1]\n",
    "        historic.loc[historic.index[-1], \"median_price\"] = predicted_price\n",
    "\n",
    "    # Step 8: Clean & combine outputs\n",
    "    for df_ in [historic, gf]:\n",
    "        if \"area_name_en\" in df_.columns:\n",
    "            df_.drop(columns=[\"area_name_en\"], inplace=True)\n",
    "\n",
    "    final_df = pd.concat([historic, gf], ignore_index=True)\n",
    "    final_df = final_df[[\"month\", \"median_price\"]]\n",
    "    final_df = final_df.sort_values(\"month\").reset_index(drop=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "271738b8-f97f-4fd8-9796-79e081e383fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Model Prediction: 8726.947816469816\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 19\u001b[0m\n\u001b[0;32m      6\u001b[0m historic_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistorical_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m input_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea_name_en\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAl_Barsha_South_Fifth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocedure_area\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melevator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m }\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m predict_with_area(input_data, forecast_df, historic_df)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "Cell \u001b[1;32mIn[47], line 130\u001b[0m, in \u001b[0;36mpredict_with_area\u001b[1;34m(input_data, forecast_df, historic_df)\u001b[0m\n\u001b[0;32m    128\u001b[0m final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([historic, gf], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m final_df \u001b[38;5;241m=\u001b[39m final_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_price\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m--> 130\u001b[0m final_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:7200\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ascending, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m   7198\u001b[0m         ascending \u001b[38;5;241m=\u001b[39m ascending[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 7200\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m nargsort(\n\u001b[0;32m   7201\u001b[0m         k, kind\u001b[38;5;241m=\u001b[39mkind, ascending\u001b[38;5;241m=\u001b[39mascending, na_position\u001b[38;5;241m=\u001b[39mna_position, key\u001b[38;5;241m=\u001b[39mkey\n\u001b[0;32m   7202\u001b[0m     )\n\u001b[0;32m   7203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   7204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\sorting.py:439\u001b[0m, in \u001b[0;36mnargsort\u001b[1;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[0;32m    437\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    438\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 439\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[non_nans\u001b[38;5;241m.\u001b[39margsort(kind\u001b[38;5;241m=\u001b[39mkind)]\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[0;32m    441\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5️⃣ USER INPUT TEST\n",
    "# ============================================================\n",
    "\n",
    "forecast_df = pd.read_csv(\"Sarima_forecast_6M.csv\")\n",
    "historic_df = pd.read_csv(\"historical_df.csv\")\n",
    "\n",
    "input_data = {\n",
    "    \"area_name_en\": \"Al_Barsha_South_Fifth\",\n",
    "    \"procedure_area\": 150,\n",
    "    \"has_parking\": 1,\n",
    "    \"floor_bin\": \"41-50\",\n",
    "    \"rooms_en\": \"4BR\",\n",
    "    \"swimming_pool\": 1,\n",
    "    \"balcony\": 1,\n",
    "    \"elevator\": 1\n",
    "}\n",
    "\n",
    "output = predict_with_area(input_data, forecast_df, historic_df)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a2680-749b-430a-b7b7-5efd148a447f",
   "metadata": {},
   "source": [
    "## 1235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aac1e2d-48b6-44a8-9f94-29ad10f6d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "# ------------------------------\n",
    "# 1️⃣ BASE DIRECTORY – MODELS PER AREA\n",
    "# ------------------------------\n",
    "# C:\\Users\\anant\\OneDrive\\Desktop\\truEstates\\Modular_code with 18 areas_RF\\area_models_rf\n",
    "BASE_DIR = os.path.join(\n",
    "    os.path.expanduser(\"~\"),\n",
    "    # \"C:\",\n",
    "    # \"Users\",\n",
    "    # \"anant\",\n",
    "    \"OneDrive\",\n",
    "    \"Desktop\",\n",
    "    \"truEstates\",\n",
    "    \"Modular_code with 18 areas_RF\"\n",
    ")\n",
    "    # \"Modular_Code\",\n",
    "    # \"Modular_code with 18 areas_RF\"\n",
    "\n",
    "AREA_DIR = os.path.join(BASE_DIR, \"area_models_rf\") \n",
    "COLUMNS_DIR = os.path.join(BASE_DIR, \"training_columns\")  \n",
    "# ------------------------------\n",
    "# 2️⃣ LOAD TRAINING COLUMNS\n",
    "# ------------------------------\n",
    "def load_columns(area_name_en):\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "    for f in os.listdir(COLUMNS_DIR):\n",
    "        if f.lower() == f\"model_columns_{area_name_en}.pkl\".lower() or \\\n",
    "           f.lower() == f\"model_columns_{area_name_en.replace(' ', '_')}.pkl\".lower():\n",
    "            with open(os.path.join(COLUMNS_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    raise FileNotFoundError(f\"❌ model_columns file not found for area '{area_name_en}'\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3️⃣ LOAD MODEL\n",
    "# ------------------------------\n",
    "def load_model(area_name_en):\n",
    "    area_name_en = area_name_en.replace(\"_\", \" \").strip()\n",
    "    for f in os.listdir(AREA_DIR):\n",
    "        if f.lower() == f\"rf_model_{area_name_en}.pkl\".lower() or \\\n",
    "           f.lower() == f\"rf_model_{area_name_en.replace(' ', '_')}.pkl\".lower():\n",
    "            with open(os.path.join(AREA_DIR, f), \"rb\") as file:\n",
    "                return pickle.load(file)\n",
    "    raise FileNotFoundError(f\"❌ Model file not found for area '{area_name_en}'\")\n",
    "\n",
    "# ------------------------------\n",
    "# 4️⃣ PREDICTION FUNCTION\n",
    "# ------------------------------\n",
    "def predict_with_area(input_data):\n",
    "    lowess_frac=0.03\n",
    "    forecast_df = pd.read_csv(\"Sarima_forecast_6M.csv\")\n",
    "    historic_df = pd.read_csv(\"historical_df.csv\")\n",
    "    area = input_data[\"area_name_en\"].replace(\"_\", \" \").strip()\n",
    "\n",
    "    # Step 1: Load model + expected columns\n",
    "    train_columns = load_columns(area)\n",
    "    model = load_model(area)\n",
    "\n",
    "    # Step 2: One-hot encode input\n",
    "    temp = pd.DataFrame([input_data])\n",
    "    temp[\"area_name_en\"] = area\n",
    "    temp = pd.get_dummies(temp)\n",
    "    for col in train_columns:\n",
    "        if col not in temp.columns:\n",
    "            temp[col] = 0\n",
    "    temp = temp[train_columns]\n",
    "\n",
    "    # Step 3: Predict median price\n",
    "    predicted_price = model.predict(temp)[0]\n",
    "    print(\"Raw Model Prediction:\", predicted_price)\n",
    "\n",
    "    # Step 4: Prepare forecast dataframe for area\n",
    "    forecast_area = forecast_df[forecast_df[\"area_name_en\"].replace(\"_\", \" \") == area].copy()\n",
    "    forecast_area[\"median_price\"] = predicted_price * forecast_area[\"growth_factor\"]\n",
    "    forecast_area = forecast_area[[\"month\", \"median_price\"]]\n",
    "\n",
    "    # Step 5: Prepare historic dataframe for area\n",
    "    historic_area = historic_df[historic_df[\"area_name_en\"].replace(\"_\", \" \") == area].copy()\n",
    "\n",
    "    # Step 6: Apply LOWESS smoothing on historic using index as x\n",
    "    if not historic_area.empty:\n",
    "        historic_area = historic_area.sort_values(\"month\").reset_index(drop=True)\n",
    "        x = historic_area.index.values  # use index for LOWESS\n",
    "        smoothed = lowess(\n",
    "            endog=historic_area[\"median_price\"].values,\n",
    "            exog=x,\n",
    "            frac=lowess_frac\n",
    "        )\n",
    "        historic_area[\"median_price\"] = smoothed[:, 1]\n",
    "\n",
    "        # Replace last historic value with first forecast median price\n",
    "        if not forecast_area.empty:\n",
    "            historic_area.loc[historic_area.index[-1], \"median_price\"] = forecast_area.iloc[0][\"median_price\"]\n",
    "\n",
    "    # Step 7: Combine historic + forecast\n",
    "    final_df = pd.concat([historic_area[[\"month\", \"median_price\"]], forecast_area], ignore_index=True)\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "46710a79-82f5-4fb8-8d96-2f2d229c3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_list =['Al Barsha South Fifth',\n",
    " 'Al Barsha South Fourth',\n",
    " 'Al Barshaa South Third',\n",
    " 'Al Hebiah Fourth',\n",
    " 'Al Khairan First',\n",
    " 'Al Merkadh',\n",
    " 'Al Thanyah Fifth',\n",
    " 'Al Yelayiss 2',\n",
    " 'Burj Khalifa',\n",
    " 'Business Bay',\n",
    " 'Hadaeq Sheikh Mohammed Bin Rashid',\n",
    " 'Jabal Ali First',\n",
    " 'Madinat Al Mataar',\n",
    " 'Madinat Dubai Almelaheyah',\n",
    " 'Marsa Dubai',\n",
    " \"Me'Aisem First\",\n",
    " 'Nadd Hessa',\n",
    " 'Wadi Al Safa 5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f89de6-7db4-4dd4-9803-57635ee83c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anant\\OneDrive\\Desktop\\truEstates\\.conda\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\anant\\OneDrive\\Desktop\\truEstates\\.conda\\lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Model Prediction: 14245.759458443812\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>median_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>11774.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>10759.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2025-10-01</td>\n",
       "      <td>10778.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2025-11-01</td>\n",
       "      <td>14714.122270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2025-12-01</td>\n",
       "      <td>14714.122270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2026-01-01</td>\n",
       "      <td>14962.070428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2026-02-01</td>\n",
       "      <td>15093.332534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2026-03-01</td>\n",
       "      <td>15162.821820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2026-04-01</td>\n",
       "      <td>15199.608986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2026-05-01</td>\n",
       "      <td>15219.083867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         month  median_price\n",
       "67  2025-08-01  11774.150000\n",
       "68  2025-09-01  10759.825000\n",
       "69  2025-10-01  10778.270000\n",
       "70  2025-11-01  14714.122270\n",
       "71  2025-12-01  14714.122270\n",
       "72  2026-01-01  14962.070428\n",
       "73  2026-02-01  15093.332534\n",
       "74  2026-03-01  15162.821820\n",
       "75  2026-04-01  15199.608986\n",
       "76  2026-05-01  15219.083867"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = {\n",
    "    \"area_name_en\":  'Al Hebiah Fourth',\n",
    "    \"procedure_area\": 70,\n",
    "    \"has_parking\": 1,\n",
    "    \"floor_bin\": \"11-20\",\n",
    "    \"rooms_en\": \"1BR\",\n",
    "    \"swimming_pool\": 1,\n",
    "    \"balcony\": 1,\n",
    "    \"elevator\": 1,\n",
    "    \"metro\" : 0\n",
    "}\n",
    "\n",
    "final_df = predict_with_area(input_data)\n",
    "\n",
    "final_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adec03-3a2e-49af-8d94-7db43461c18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
